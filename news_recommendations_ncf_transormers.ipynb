{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "news-recommendations-ncf-transormers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcNcg2aK6YtG4xwcvDKgMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moinudeen/neural-collaborative-filtering-news-recommendations/blob/main/news_recommendations_ncf_transormers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9oevichC01e"
      },
      "source": [
        "## Package Installation and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebTmBDSV1006",
        "outputId": "2e09bb2c-e23e-46cc-8cfb-9e8c442a8c0b"
      },
      "source": [
        "!pip install -U sentence-transformers pytorch-lightning scikit-plot wordcloud"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 33.7 MB/s \n",
            "\u001b[?25hCollecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n",
            "\u001b[K     |████████████████████████████████| 366 kB 42.8 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 19.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 35.8 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting tensorboard!=2.5.0,>=2.2.0\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (7.1.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (57.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, future\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126709 sha256=c35b6c103f191303fcdbfe7ef859ae72e86a18ea9efe130920606021e2a96cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/0f/faafd427f705c4b012274ba60d9a91d75830306811e1355293\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=5682254a6b9576e0bd6e92e5845c6ee7d0dfd0e9afee00f35e29f1d9725e5a2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built sentence-transformers future\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sacremoses, pyyaml, huggingface-hub, fsspec, aiohttp, transformers, torchmetrics, tensorboard, sentencepiece, pyDeprecate, future, wordcloud, sentence-transformers, scikit-plot, pytorch-lightning\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: wordcloud\n",
            "    Found existing installation: wordcloud 1.5.0\n",
            "    Uninstalling wordcloud-1.5.0:\n",
            "      Successfully uninstalled wordcloud-1.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 huggingface-hub-0.0.12 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 pyyaml-5.4.1 sacremoses-0.0.45 scikit-plot-0.3.7 sentence-transformers-2.0.0 sentencepiece-0.1.96 tensorboard-2.4.1 tokenizers-0.10.3 torchmetrics-0.4.1 transformers-4.9.0 wordcloud-1.8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQHvdZCW2ZGF"
      },
      "source": [
        "\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import scikitplot as skplt\n",
        "\n",
        "np.random.seed(123)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbyGyersIDU1"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mndIUl52DfHs"
      },
      "source": [
        "## Download the data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZkrbH3dVFBA"
      },
      "source": [
        "! wget https://inspire-data-challenge.s3.amazonaws.com/user_news_clicks.csv "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0c-zNWXVI_H"
      },
      "source": [
        "! wget https://inspire-data-challenge.s3.amazonaws.com/news_text.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWA3adojVP41"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJi8PKXZDPAT"
      },
      "source": [
        "## Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3anGm_VUrT"
      },
      "source": [
        "news_df = pd.read_csv(\"news_text.csv\", sep=\"\\t\")\n",
        "news_df['title'] = news_df['title'].fillna(\"\")\n",
        "news_df['title'] = news_df['title'].str.lower()\n",
        "news_df['title'] = news_df.apply(lambda z: z.get(\"title\", \"\")+\".\" if z.get(\"title\") and z.get(\"title\", \"\")[-1] not in string.punctuation else z.get(\"title\"), axis=1)\n",
        "news_df['abstract'] = news_df['abstract'].fillna(\"\")\n",
        "news_df['abstract'] = news_df['abstract'].str.lower()\n",
        "news_df['text'] = news_df.apply(lambda z: z.get(\"title\", \"\")+ \" \" + z.get(\"abstract\", \"\"), axis=1)\n",
        "news_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFok8U3JjMzK"
      },
      "source": [
        "news_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9lrf_AEGGbG"
      },
      "source": [
        "print(\"unique items: \", len(news_df.news_id.unique()))\n",
        "print(\"unique categories: \", len(news_df.category.unique()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ClKc4IHDxN-"
      },
      "source": [
        "news_df.category.value_counts().plot(kind='bar', title='distribution of category values', figsize=(20, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2wIXzELWT1W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yggiVtQWkP8q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUDlB1GGW4HL"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCUt5i6kXGPv"
      },
      "source": [
        "clicks_df = pd.read_csv(\"user_news_clicks.csv\")\n",
        "clicks_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17RqDc7AvDYF"
      },
      "source": [
        "print(\"unique users: \", len(clicks_df.user_id.unique()))\n",
        "print(\"unique items: \", len(clicks_df.item.unique()))\n",
        "print(\"unique interactions: \",len(clicks_df.click.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQGpkQOmKkKb"
      },
      "source": [
        "clicks_df.shape, clicks_df.drop_duplicates(subset=[\"user_id\", \"item\", \"click\"]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uw0KZNNJbCG"
      },
      "source": [
        "clicks_df.click.value_counts().plot(kind='bar', title='distribution of non-clicks vs clicks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNIiT6w2JKd6"
      },
      "source": [
        "## Encode news articles text with embeddings from SentenceTransformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8shqsHG4JLOZ"
      },
      "source": [
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('average_word_embeddings_komninos')\n",
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = news_df['text'].tolist()\n",
        "\n",
        "# run the encoder\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "news_df['text_embedding'] = embeddings.tolist()\n",
        "\n",
        "print(embeddings.shape)\n",
        "\n",
        "news_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUeMZJ3_58kw"
      },
      "source": [
        "## Encode news category column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FecqQx46AL6"
      },
      "source": [
        "news_category_encoder = LabelEncoder()\n",
        "news_df['category_encoded'] = news_category_encoder.fit_transform(news_df[\"category\"])\n",
        "news_dict = {r['news_id']: r for r in news_df.to_dict(\"rows\")}\n",
        "len(news_dict)\n",
        "news_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ty26G_P683w"
      },
      "source": [
        "## Encode user_id and item_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqyLvVwrS_Lo"
      },
      "source": [
        "label_encoders = {}\n",
        "label_encoders[\"user_id\"] = LabelEncoder()\n",
        "label_encoders[\"item\"] = LabelEncoder()\n",
        "clicks_df[\"user_id_encoded\"] = label_encoders[\"user_id\"].fit_transform(clicks_df[\"user_id\"])\n",
        "clicks_df[\"item_id_encoded\"] = label_encoders[\"item\"].fit_transform(clicks_df[\"item\"])\n",
        "clicks_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANDCkqqp7Kgh"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "- Random Sampling data for faster training with limited hardware resources\n",
        "- There are lots of user-item interactions with more than one event, to keep it simple dropping these duplicates from the data as we are using these interactions as implicit feedback. This will also prevent training data leakage \n",
        "- Train and Test split will be done randomly as we don't have any timestamp values for the user-item interactions to split based on chronology of events.\n",
        "- Not going for negative sampling to keep it simple here. The distribution of clicks vs non-clicks looks balanced already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QMX0kTJOEGS"
      },
      "source": [
        "traindf = clicks_df.drop_duplicates(subset=[\"user_id\", \"item\", \"click\"])\n",
        "traindf = traindf.sample(frac=0.4)\n",
        "testdf = traindf.sample(frac=0.025)\n",
        "traindf = traindf.drop(testdf.index)\n",
        "traindf.shape, testdf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPRebwwrT5uX"
      },
      "source": [
        "traindf.click.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3qnhVw6ImN8"
      },
      "source": [
        "testdf.click.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvzp62GYB2W2"
      },
      "source": [
        "# Modelling\n",
        "- using neural collaborating filtering approach "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrDXI9JUAN9"
      },
      "source": [
        "class MINDTrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Training MIND dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, interactions, all_news_ids):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        interactions (pd.DataFrame): Dataframe containing the interactions\n",
        "        all_news_ids (dict): dict containing all news ids and its metadata\n",
        "        \"\"\"\n",
        "        self.users, self.items, self.item_cats, self.labels = self.get_dataset(interactions, all_news_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.item_cats[idx], self.labels[idx]\n",
        "\n",
        "    def get_dataset(self, interactions, all_news_ids):\n",
        "        users, items, labels, item_cats = [], [], [], []\n",
        "        user_item_set = set(zip(interactions['user_id_encoded'], interactions['item'], interactions['click']))\n",
        "\n",
        "        for u, i, l in user_item_set:\n",
        "            users.append(u)\n",
        "            items.append(all_news_ids[i]['text_embedding'])\n",
        "            item_cats.append(all_news_ids[i]['category_encoded'])\n",
        "            labels.append(l)\n",
        "    \n",
        "        return torch.tensor(users), torch.tensor(items), torch.tensor(item_cats), torch.tensor(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaTYF0XCUGbW"
      },
      "source": [
        "class NCF(pl.LightningModule):\n",
        "    \"\"\" \n",
        "    Neural Collaborative Filtering (NCF)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_users, num_item_cats, text_embedding_dim, interactions, all_news_ids, embedding_hidden_dim=16):\n",
        "        \"\"\"\n",
        "         Args:\n",
        "            num_users (int): Number of unique users\n",
        "            num_item_cats (int): Number of unique item cats\n",
        "            text_embedding_dim (int): dimensions of the text embedding\n",
        "            interactions (pd.DataFrame): Dataframe containing the news clicks\n",
        "            all_news_ids (dict): dict containing all news ids and its metadata\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_hidden_dim)\n",
        "        self.item_cat_embedding = nn.Embedding(num_embeddings=num_item_cats, embedding_dim=embedding_hidden_dim)\n",
        "        self.input_feature_shape = text_embedding_dim+embedding_hidden_dim+embedding_hidden_dim\n",
        "        print(self.input_feature_shape)\n",
        "        self.fc1 = nn.Linear(in_features=self.input_feature_shape, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.interactions = interactions\n",
        "        self.all_news_ids = all_news_ids\n",
        "        \n",
        "    def forward(self, user_input, item_cat_input, item_embedding_input):\n",
        "        \n",
        "        # Compute embeddings \n",
        "        user_embedded = self.user_embedding(user_input)\n",
        "        item_cat_embedded = self.item_cat_embedding(item_cat_input)\n",
        "\n",
        "        # Concat the embeddings\n",
        "        vector = torch.cat([user_embedded, item_cat_embedded, item_embedding_input], dim=-1)\n",
        "\n",
        "        # Pass through fully connected\n",
        "        out = nn.ReLU()(self.fc1(vector))\n",
        "        out = nn.ReLU()(self.fc2(out))\n",
        "        out = nn.ReLU()(self.fc3(out))\n",
        "\n",
        "        # Output layer\n",
        "        pred = nn.Sigmoid()(self.output(out))\n",
        "\n",
        "        return pred\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # compute the loss\n",
        "        user_input, item_embedding, item_cat_input, labels = batch\n",
        "        predicted_labels = self(user_input, item_cat_input, item_embedding)\n",
        "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters())\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(MINDTrainDataset(self.interactions, self.all_news_ids),\n",
        "                          batch_size=512, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcW3-nMgKHeQ"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtjdO7H9UOu2"
      },
      "source": [
        "num_users = max(traindf['user_id_encoded'].max()+1, testdf['user_id_encoded'].max()+1)\n",
        "num_items_cat = news_df['category_encoded'].max()+1\n",
        "text_embedding_dim = embeddings.shape[1]\n",
        "\n",
        "model = NCF(num_users, num_items_cat, text_embedding_dim, traindf, news_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_smiy5rZgrim"
      },
      "source": [
        "text_embedding_dim, num_users, num_items_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp-oN2P1dTNH"
      },
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"NCF_SBERT\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8IOj248UQ4u"
      },
      "source": [
        "trainer = pl.Trainer(max_epochs=1, reload_dataloaders_every_epoch=True, progress_bar_refresh_rate=50, logger=logger, checkpoint_callback=False)\n",
        "\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8B7d-ERepjL"
      },
      "source": [
        "test_item_embeddings = [news_dict.get(i).get(\"text_embedding\") for i in testdf['item'].values]\n",
        "test_itemcat_embeddings = [news_dict.get(i).get(\"category_encoded\") for i in testdf['item'].values]\n",
        "\n",
        "\n",
        "trainer.logger.experiment.add_graph(model, input_to_model=(torch.tensor(testdf['user_id_encoded'].values[0]), torch.tensor(test_itemcat_embeddings[0]), torch.tensor(test_item_embeddings[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3A2DoLif0dD"
      },
      "source": [
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44nBrq90Wvbn"
      },
      "source": [
        "%tensorboard --logdir tb_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs86TWb9J-LP"
      },
      "source": [
        "# Evaluating the model\n",
        "\n",
        "\n",
        "\n",
        "*   Calculate the classification metrics precision, recall and f1 score with a default threshold of 0.5\n",
        "*   Plot confusion matrix, PR Curves and ROC curves.\n",
        "*   From the metrics, we can observe that the precision is high overall but the recall is lower for clicks==1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZ81DNkUVnn"
      },
      "source": [
        "\n",
        "predicted_labels = np.squeeze(model(torch.tensor(testdf['user_id_encoded'].values), torch.tensor(test_itemcat_embeddings), torch.tensor(test_item_embeddings)).detach().numpy())\n",
        "\n",
        "testdf['prediction_conf1'] = predicted_labels.tolist()\n",
        "testdf['prediction_conf0'] = (1-predicted_labels).tolist()\n",
        "testdf['prediction'] = testdf['prediction_conf1'].apply(lambda z: 1 if z>0.5 else 0)\n",
        "testdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dSoi9KUWGx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cmd = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
        "    cmd.plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS9xLqwkUXAK"
      },
      "source": [
        "plot_confusion_matrix(testdf['click'], testdf['prediction'], [1, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27S2hVYzUY2L"
      },
      "source": [
        "print(classification_report(testdf['click'], testdf['prediction'], [1, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjHWZQYxC3xy"
      },
      "source": [
        "\n",
        "skplt.metrics.plot_precision_recall(testdf['click'].values, testdf[['prediction_conf0', 'prediction_conf1']].values, classes_to_plot=[0, 1], figsize=(10,10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOJJOdZBPrL0"
      },
      "source": [
        "skplt.metrics.plot_roc_curve(testdf['click'].values, testdf[['prediction_conf0', 'prediction_conf1']].values, figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGU94rXwDhar"
      },
      "source": [
        "## Visualizing outputs\n",
        "- doing some sanity checks to figure if the model outputs make sense\n",
        "   - randomly taking an user id and checking the titles from train clicks and predicted clicks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUFrqf5SDgT1"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_wordcloud_for_user(text_list):\n",
        "\n",
        "    stopwords = set(STOPWORDS).union([np.nan, 'NaN', 'S'])\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "                   max_words=50000,\n",
        "                   min_font_size =12,\n",
        "                   max_font_size=50,\n",
        "                   relative_scaling = 0.9,\n",
        "                   stopwords=set(STOPWORDS),\n",
        "                   normalize_plurals= True\n",
        "    )\n",
        "\n",
        "    clean_titles = [word for word in text_list if word not in stopwords]\n",
        "    title_wordcloud = wordcloud.generate(' '.join(clean_titles))\n",
        "\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.imshow(title_wordcloud, interpolation='bilinear',)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg_o5GTn-ve-"
      },
      "source": [
        "# traindf.user_id.value_counts()[44300:44305]\n",
        "ii = traindf[(traindf['user_id']==\"U84756\") & (traindf['click']==1)]['item'].unique()\n",
        "tlist = [news_dict[i]['title'] for i in ii]\n",
        "# tlist\n",
        "news_df[news_df['news_id'].isin(ii)]['category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbuyGjNOLHrz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30BP3ZgXJmbW"
      },
      "source": [
        "get_wordcloud_for_user(tlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmgu9bloI4WZ"
      },
      "source": [
        "ii = testdf[(testdf['user_id']==\"U84756\") & (testdf['prediction']==1)]['item'].unique()\n",
        "news_df[news_df['news_id'].isin(ii)]['category'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvfXdeYPIyva"
      },
      "source": [
        "# testdf[testdf['user_id']==\"U84756\"]\n",
        "tlist1 = [news_dict[i]['title'] for i in ii]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XcA1_UtKoj3"
      },
      "source": [
        "get_wordcloud_for_user(tlist1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeq4tXXm_Ggj"
      },
      "source": [
        "# Further Improvements\n",
        "\n",
        "- More feature engineering: user-category affinity score, chronological based train-test split and evaluation, etc\n",
        "- Hyperparamter tuning: finding the best values for different hyperparams like text embedding method, Neural Net layers and hidden sizes, batch size, epochs, optimizers, etc.\n",
        "- Finetuning the prediction probabilty thresholds to find the right balance. i.e, Precision-Recall tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abSvlUgTCX-X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}